1、今天到现在为止实战课程的访问量
2、到现在为止从搜索引擎引流过来的访问量

定时执行：Linux crontab

网址：https://tool.lu/crontab

每一分钟执行一次的crontab表达式： */1 * * * *

vi log_generator.sh

python generate_log.py

*/1 * * * * /home/hadoop/data/project/log_generator.sh

将上面一句 放到 crontab -e 中    每一分钟产生一批日志



对接Python日志产生器输出的日志到flume
streaming_project.conf

选型：access.log ==> 控制台输出
       exec
       memory
       logger
agent:
       exec-memory-logger.sources = exec-source
       exec-memory-logger.sinks = logger-sink
       exec-memory-logger.channels = memory-channel

       exec-memory-logger.sources.exec-source.type = exec
       exec-memory-logger.sources.exec-source.command = tail -F /home/hadoop/data/project/logs/access.log
       exec-memory-logger.sources.exec-source.shell = /bin/sh -c

       exec-memory-logger.channels.memory-channel.type = memory

       exec-memory-logger.sinks.logger-sink.type = logger

        exec-memory-logger.sources.exec-source.channels = memory-channel
        exec-memory-logger.sinks.logger-sink.channels = memory-channel

        flume-ng agent \
        --conf $FLUME_HOME/conf \
        --conf-file /home/hadoop/data/project/streaming_project.conf \
        --name exec-memory-logger \
        -Dflume.root.logger=INFO,console


日志==>flume==>kafka

对接kafka：
 启动zk -> cd $ZK_HOME/bin  ./zkServer.sh start
 启动kafka -> cd $KAFKA_HOME/bin ./kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
 修改flume配置文件，使flume数据sink到kafka中

streaming_project2.conf
 agent:
        exec-memory-kafka.sources = exec-source
        exec-memory-kafka.sinks = kafka-sink
        exec-memory-kafka.channels = memory-channel

        exec-memory-kafka.sources.exec-source.type = exec
        exec-memory-kafka.sources.exec-source.command = tail -F /home/hadoop/data/project/logs/access.log
        exec-memory-kafka.sources.exec-source.shell = /bin/sh -c

        exec-memory-kafka.channels.memory-channel.type = memory

        exec-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink
        exec-memory-kafka.sinks.kafka-sink.brokerList = hadoop00:9092
        exec-memory-kafka.sinks.kafka-sink.topic = streamingtopic
        exec-memory-kafka.sinks.kafka-sink.batchsize = 5
        exec-memory-kafka.sinks.kafka-sink.requiredAcks = 1

         exec-memory-kafka.sources.exec-source.channels = memory-channel
         exec-memory-kafka.sinks.kafka-sink.channels = memory-channel



        cd $KAFKA_HOME/bin ./kafka-console-consumer.sh --zookeeper hadoop000:2181 --topic streamingtopic

        flume-ng agent \
                 --conf $FLUME_HOME/conf \
                 --conf-file /home/hadoop/data/project/streaming_project2.conf \
                 --name exec-memory-kafka \
                 -Dflume.root.logger=INFO,console